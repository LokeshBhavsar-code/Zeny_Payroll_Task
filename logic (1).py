# -*- coding: utf-8 -*-
"""Logic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3uDolVnsWxsNhMKYKMtkdFuHrvG4_WT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def create_dummy_data(n=2000):

    data = {
        'emp_id': range(1, n + 1),
        'attendance_avg': np.random.randint(10, 27, n),      # avg working days/month
        'leaves_yearly': np.random.randint(0, 45, n),        # leaves per year
        'hike_percent': np.random.normal(10, 5, n),          # salary hike %
        'current_salary': np.random.randint(30000, 120000, n),


        'performance_rating': np.random.randint(1, 6, n),    # 1–5 rating
        'years_experience': np.random.randint(0, 31, n)      # 0–30 years
    }

    df = pd.DataFrame(data)

    # Business constraints
    df['hike_percent'] = df['hike_percent'].clip(lower=0)

    return df

# Create dataset with 2000 rows
df = create_dummy_data()

df.head()

df['risk_label'] = (
    ((df['leaves_yearly'] > 30) & (df['hike_percent'] > 15)) |
    (df['attendance_avg'] < 15)
).astype(int)

np.random.seed(42)

flip_indices = np.random.choice(df.index, size=50, replace=False)
df.loc[flip_indices, 'risk_label'] = 1 - df.loc[flip_indices, 'risk_label']

print(df['risk_label'].value_counts())
df.head()

#  correlations
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Matrix")
plt.show()

# Visualizing the risk distribution
sns.countplot(x='risk_label', data=df)
plt.title("Class Balance Check")
plt.show()

# Feature Engineering
# 1. Cost per working day: Helps identify overpaid employees relative to attendance
df['cost_per_day'] = df['current_salary'] / (df['attendance_avg'] * 12)

# 2. Leave ratio
df['leave_ratio'] = df['leaves_yearly'] / 365

print("New features added.")
df.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Preparing X and y
features = ['attendance_avg', 'leaves_yearly', 'hike_percent', 'current_salary', 'cost_per_day']
X = df[features]
y = df['risk_label']

# We need to scale data for Logistic Regression
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models_dict = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),
    "XGBoost (Gradient Boosting)": GradientBoostingClassifier(random_state=42)
}

results = []

for name, model in models_dict.items():
    print(f"Training {name}...")

    if name == "Logistic Regression":
        model.fit(X_train_scaled, y_train)
        preds = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

    acc = accuracy_score(y_test, preds)
    results.append({'Model Name': name, 'Accuracy': acc})

    print(f"\n--- {name} Report ---")
    print(classification_report(y_test, preds))

# Show final comparison
results_df = pd.DataFrame(results)
results_df

# Random Forest
rf_model = models_dict['Random Forest']
importances = rf_model.feature_importances_

# Plotting
feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)
feat_imp.plot(kind='bar', title='Feature Importance for Risk Scoring')
plt.ylabel('Score')
plt.show()

